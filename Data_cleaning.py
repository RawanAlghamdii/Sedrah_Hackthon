from fastapi import FastAPI, UploadFile, File
import pandas as pd
import ollama
import uvicorn
import socket
import time
import io
import asyncio

app = FastAPI(title="ğŸŒ¿ Plant Recommender API")

@app.post("/recommend/")
async def recommend(file: UploadFile = File(...)):
    print("ğŸ”¹ Starting recommend endpoint")
    try:
        contents = await file.read()
        print("ğŸ”¹ File read successfully")
        df = pd.read_csv(io.StringIO(contents.decode("utf-8")))
        #df.drop(columns=["Latitude", "Longitude"], inplace=True)
        print(f"ğŸ”¹ CSV loaded, shape: {df.shape}")
    except Exception as e:
        print(f"âŒ Error reading CSV: {e}")
        return {"error": "âŒ Failed to read the CSV file", "details": str(e)}

    async def get_recommendation(row):
        prompt = f"""
You are an agricultural assistant AI.
Recommend the best plant to grow in this environment based on the following surface features:

- Soil Type: {row.get('Soil_Type', 'unknown')}
- Desertification Level: {row.get('Desertification_Level', 'unknown')}
- NDVI (Vegetation Index): {row.get('NDVI', 'unknown')}
- NDWI (Water Index): {row.get('NDWI', 'unknown')}
- Land Surface Temperature (Day): {row.get('LST_Day', 'unknown')}
- Elevation: {row.get('DEM', 'unknown')}

Consider only these plants for recommendation:
Date Palm, Tamarisk, Acacia tortilis, Acacia ehrenbergiana, Haloxylon salicornicum, 
Rhanterium epapposum, Ziziphus spina-christi, Calligonum comosum, Peganum harmala, 
Suaeda aegyptiaca, Lathyrus aphaca.

Give a plant name and short explain why it is suitable, less than 15 words.
"""

        print(f"ğŸ”¹ Sending prompt for row index {row.name}")
        try:
            response = await asyncio.to_thread(
                ollama.chat,
                model="qwen2:7b",
                messages=[
                    {"role": "system", "content": "You are a smart farming assistant."},
                    {"role": "user", "content": prompt}
                ]
            )
            print(f"ğŸ”¹ Received response for row index {row.name}")
            return response['message']['content'].strip()
        except Exception as e:
            print(f"âŒ Error in ollama.chat for row index {row.name}: {e}")
            return f"âŒ Error getting recommendation: {e}"

    print("ğŸ”¹ Starting to process rows concurrently")
    
    # Ø®Ø° Ø£ÙˆÙ„ 1000 ØµÙ ÙÙ‚Ø·
    df_subset = df.head(200).copy()

    # Ø£Ù†Ø´Ø¦ Ø§Ù„Ù…Ù‡Ø§Ù… Ù„Ù„ØªÙˆØµÙŠØ© Ø¹Ù„Ù‰ ÙƒÙ„ ØµÙ
    tasks = [get_recommendation(row) for _, row in df_subset.iterrows()]

    # Ù†ÙÙ‘Ø° Ø§Ù„Ù…Ù‡Ø§Ù… ÙƒÙ„Ù‡Ø§ Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©
    results = await asyncio.gather(*tasks)

    # Ø®Ø²Ù‘Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ DataFrame Ø¬Ø¯ÙŠØ¯
    df_result = df_subset.copy()
    df_result["Recommended_Plant"] = results
    print("ğŸ”¹ Added recommendations to dataframe")

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù CSV
    df_result.to_csv(r"C:\Users\96653\Downloads\recommendations.csv", index=False)
    print("ğŸ”¹ Saved recommendations to recommendations.csv")

    # Ø¥Ø¹Ø§Ø¯Ø© Ø£ÙˆÙ„ 5 ØµÙÙˆÙ ÙƒØ§Ø³ØªØ¬Ø§Ø¨Ø© API
    return df_result.head(5).to_dict(orient="records")


def check_ollama_ready(host='localhost', port=11434):
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(5)
    result = sock.connect_ex((host, port))
    sock.close()
    return result == 0


if __name__ == "__main__":
    while not check_ollama_ready():
        print("ğŸ•’ Waiting for Ollama to start on port 11434...")
        time.sleep(3)

    print("âœ… Ollama is ready. Pulling the model if needed...")
    ollama.pull("qwen2:7b")

    print("ğŸš€ API running at http://127.0.0.1:8000/docs")
    uvicorn.run("sedrah:app", host="0.0.0.0", port=8000)
